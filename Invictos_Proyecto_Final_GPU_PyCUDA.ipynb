{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Invictos_Proyecto_Final_GPU_PyCUDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vvFYuvzkGuAP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Configuración CUDA"
      ]
    },
    {
      "metadata": {
        "id": "OcXj6Xc8Gxxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yI7yXBkLGz4v",
        "colab_type": "code",
        "outputId": "82702177-2892-4650-d4b3-61f925601f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!/usr/local/cuda/bin/nvcc --version"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Tue_Jun_12_23:07:04_CDT_2018\n",
            "Cuda compilation tools, release 9.2, V9.2.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DzqX2O-4JrxS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5485a8c4-0aac-477e-fead-f84942863954"
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-534c92b1-d0c4-521f-c011-5c5bb20afca4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XRLHjbUvG1N3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gT2tTPvcbG4Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Specs"
      ]
    },
    {
      "metadata": {
        "id": "6EqCVK_m-MaP",
        "colab_type": "code",
        "outputId": "7ba9dac4-9491-4fa6-83d0-57f39d60b864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!df -h"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G  7.6G  333G   3% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G  8.0K  6.4G   1% /var/colab\n",
            "/dev/sda1       365G   11G  355G   3% /opt/bin\n",
            "shm             6.0G  4.0K  6.0G   1% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Jfi_UWd-Qdu",
        "colab_type": "code",
        "outputId": "7ef322e3-cae6-416b-a44e-1dbfebeadd1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NjEslxUqbFVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "OOQfHRmNbE5e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gzip\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyCUDA imports\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import pycuda.gpuarray as gpuarray\n",
        "from pycuda import cumath, gpuarray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kDLeHIHh2mFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Forward"
      ]
    },
    {
      "metadata": {
        "id": "EmVYBfJmntc-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "convolution_mod = SourceModule(\"\"\"\n",
        "    __global__ void convolution(float *image,\n",
        "                                int n_c,\n",
        "                                int in_dim,\n",
        "                                float* filt, \n",
        "                                int n_f,\n",
        "                                int n_c_f,\n",
        "                                int f,\n",
        "                                int bias,\n",
        "                                int stride) \n",
        "    {\n",
        "\n",
        "      int out_dim = ((in_dim - f)/stride)+1);\n",
        "      const int bytes = n_f * out_dim * out_dim;\n",
        "      \n",
        "      float* out;\n",
        "      for(int i=0; i < bytes; i++) {\n",
        "        out[i] = 0;\n",
        "      }\n",
        "      \n",
        "      for(int i=0; i<n_f; i++) {\n",
        "        int curr_y = 0;\n",
        "        int out_y = 0;\n",
        "        \n",
        "        while(curr_y + f <= in_dim) {\n",
        "          int curr_x = 0;\n",
        "          int out_x = 0;\n",
        "          \n",
        "          while(curr_x + f <= in_dim) {\n",
        "            out[i*out_y*out_x] = 1;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        " \n",
        "    };\n",
        "    \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvCLnOt_Jk9m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolution(image, filt, bias, s=1):\n",
        "  '''Applies convolutional operation to the image using given filters.\n",
        "    \n",
        "  Args:\n",
        "    image: The input image\n",
        "    filt: The filters of the convolutional operation.\n",
        "    bias: A list with the (shared) biases of each filter.\n",
        "    s: Stride of the convolutional layer\n",
        "  Returns:\n",
        "    The output of passing the filters through the convolution (3D image)\n",
        "  '''\n",
        "  \n",
        "  # Get input and output shapes\n",
        "  (n_f, n_c_f, f, _) = filt.shape\n",
        "  n_c, in_dim, _ = image.shape \n",
        "  out_dim = int((in_dim - f)/s)+1\n",
        "\n",
        "  # Initialize output as zero\n",
        "  out = np.zeros((n_f, out_dim, out_dim))\n",
        "\n",
        "  # For every filter\n",
        "  for curr_f in range(n_f):\n",
        "    # Track y index of current (convolutional) and output (next) layer.\n",
        "    curr_y = 0\n",
        "    out_y = 0\n",
        "    \n",
        "    # Move filter through columns of the image\n",
        "    while curr_y + f <= in_dim:\n",
        "      # Track x index of current (convolutional) and output (next) layer.\n",
        "      curr_x = 0\n",
        "      out_x = 0\n",
        "      \n",
        "      # Move filter through rows of the image\n",
        "      while curr_x + f <= in_dim:\n",
        "        # Multiply (element-wise) filters, add result and bias\n",
        "        out[curr_f, out_y, out_x] = np.sum(filt[curr_f] * image[:,curr_y:curr_y+f, curr_x:curr_x+f]) + bias[curr_f]\n",
        "\n",
        "        curr_x += s\n",
        "        out_x += 1\n",
        "      curr_y += s\n",
        "      out_y += 1\n",
        "\n",
        "  return out\n",
        "\n",
        "def maxpool(image, f=2, s=1):\n",
        "  '''Downsamples using max-pooling\n",
        "    \n",
        "  Args:\n",
        "    image: The input image\n",
        "    f: The size of the max-pooling window\n",
        "    s: Stride of the window\n",
        "  Returns:\n",
        "    The output of max-pooling layer.\n",
        "  '''\n",
        "  \n",
        "  # Get input shape\n",
        "  n_c, h_prev, w_prev = image.shape\n",
        "  \n",
        "  # Calculate output shape\n",
        "  h = int((h_prev - f)/s)+1\n",
        "  w = int((w_prev - f)/s)+1\n",
        "  downsampled = np.zeros((n_c, h, w))\n",
        "  \n",
        "  # For every channel of the image\n",
        "  for i in range(n_c):\n",
        "    # Track y index of current (convolutional) and output (next) layer.\n",
        "    curr_y = 0\n",
        "    out_y = 0\n",
        "    \n",
        "    # Move filter through columns of the image\n",
        "    while curr_y + f <= h_prev:\n",
        "      # Track x index of current (convolutional) and output (next) layer.\n",
        "      curr_x = 0\n",
        "      out_x = 0\n",
        "      \n",
        "      # Move filter through rows of the image\n",
        "      while curr_x + f <= w_prev:\n",
        "        # Find highest value of the original image in the window position\n",
        "        downsampled[i, out_y, out_x] = np.max(image[i, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "\n",
        "        curr_x += s\n",
        "        out_x += 1\n",
        "      curr_y += s\n",
        "      out_y += 1\n",
        "  return downsampled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0oyyPpw27XE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ]
    },
    {
      "metadata": {
        "id": "QU2r8R_a2vX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def extract_data(filename, num_images, image_width=28):\n",
        "  '''Extracts images using bytestream and reshape to [num_images, height, weight]\n",
        "  '''\n",
        "  print('Extracting', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "      bytestream.read(16)\n",
        "      buf = bytestream.read(image_width * image_width * num_images)\n",
        "      data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
        "      data = data.reshape(num_images, image_width*image_width)\n",
        "      return data\n",
        "\n",
        "def extract_labels(filename, num_images):\n",
        "  '''Extracts label into a vector of [m, 1] where m is the number of images.\n",
        "  '''\n",
        "  print('Extracting', filename)\n",
        "  with gzip.open(filename) as bytestream:\n",
        "      bytestream.read(8)\n",
        "      buf = bytestream.read(1 * num_images)\n",
        "      labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
        "  return labels\n",
        "\n",
        "def initializeFilter(size, scale = 1.0):\n",
        "  \"\"\"Initialized a weight from a Gaussian normal distribution.\n",
        "  \"\"\"\n",
        "  stddev = scale/np.sqrt(np.prod(size))\n",
        "  return np.random.normal(loc = 0, scale = stddev, size = size)\n",
        "\n",
        "def initializeWeight(size):\n",
        "  \"\"\"Iitializes a weight from a standard normal distribution.\n",
        "  \"\"\"\n",
        "  return np.random.standard_normal(size=size) * 0.01\n",
        "  \n",
        "def nanargmax(arr):\n",
        "  \"\"\"Applies nanargmax for the max-pooling backpropagation.\n",
        "  \n",
        "  Example: nanargmax(np.array([0, 0, 0.01, 0, 0])) -> (2,)\n",
        "  \"\"\"\n",
        "  idx = np.nanargmax(arr)\n",
        "  idxs = np.unravel_index(idx, arr.shape)\n",
        "  return idxs\n",
        "\n",
        "def softmax(X):\n",
        "  \"\"\"Applies the softmax activation.\n",
        "  \"\"\"\n",
        "  out = np.exp(X)\n",
        "  return out/np.sum(out)\n",
        "\n",
        "def categoricalCrossEntropy(probs, label):\n",
        "  \"\"\"Calculates the loss with categorical cross entropy.\n",
        "  \"\"\"\n",
        "  return -np.sum(label * np.log(probs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHPGG9qc3Lzs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Backwards"
      ]
    },
    {
      "metadata": {
        "id": "4KXB-p9R28Pz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convolutionBackward(dconv_prev, conv_in, filt, s):\n",
        "  '''Backpropagate through the convolutional layer.\n",
        "\n",
        "  For convolution backpropagation, we need to consider the filter weights, the\n",
        "  bias and the convolutional operation. \n",
        "  \n",
        "  Each filter shares the bias, making the math simple. For each filter, we only\n",
        "  need to add all the gradients of the previous layer for the current filter.\n",
        "  \n",
        "  To update the filter values, we multiply the value of the gradient at a\n",
        "  specific point and multiply it with the value of the next layer.\n",
        "  \n",
        "  To backpropagate through the convolutional operator (pass the error to the\n",
        "  previous layer), we need to calculate the gradient for each value. To do this,\n",
        "  we just multiply the value of the gradient times the value of the filter,\n",
        "  since it's the weight.\n",
        "    \n",
        "  Args:\n",
        "    d_conv_prev: Gradient respective to this convolutional layer.\n",
        "    conv_in: The layer to which the convolutional layer is connected to.\n",
        "    filt: Size of the filter.\n",
        "    s: Stride of the convolutional layer\n",
        "  Returns:\n",
        "    The gradient of the weights, bias, and convolution operation.\n",
        "  '''\n",
        "  \n",
        "  # Get input and output shapes\n",
        "  (n_f, n_c, f, _) = filt.shape\n",
        "  (_, orig_dim, _) = conv_in.shape\n",
        "  \n",
        "  # Initialize gradients as 0\n",
        "  dout = np.zeros(conv_in.shape) \n",
        "  dfilt = np.zeros(filt.shape)\n",
        "  dbias = np.zeros((n_f, 1))\n",
        "  \n",
        "  # For every filter\n",
        "  for curr_f in range(n_f):\n",
        "    # Track y index of current (convolutional) and output (previous) layer.\n",
        "    curr_y = 0\n",
        "    out_y = 0\n",
        "\n",
        "    # Move filter through rows of image\n",
        "    while curr_y + f <= orig_dim:\n",
        "      # Track x index of current (maxpooling) and output (previous) layer.\n",
        "      curr_x = 0\n",
        "      out_x = 0\n",
        "      \n",
        "      # Move filter through columns of the image\n",
        "      while curr_x + f <= orig_dim:\n",
        "        # Calculate loss gradient of filter\n",
        "        dfilt[curr_f] += dconv_prev[curr_f, out_y, out_x] * conv_in[:, curr_y:curr_y+f, curr_x:curr_x+f]\n",
        "        \n",
        "        # Calculate loss gradient of convolution operation\n",
        "        dout[:, curr_y:curr_y+f, curr_x:curr_x+f] += dconv_prev[curr_f, out_y, out_x] * filt[curr_f] \n",
        "  \n",
        "        curr_x += s\n",
        "        out_x += 1\n",
        "\n",
        "      curr_y += s\n",
        "      out_y += 1\n",
        "\n",
        "    # Update shared bias of the filter\n",
        "    dbias[curr_f] = np.sum(dconv_prev[curr_f])\n",
        "\n",
        "  return dout, dfilt, dbias\n",
        "\n",
        "def maxpoolBackward(dpool, orig, f, s):\n",
        "  '''Backpropagate through the max-pooling layer.\n",
        "  \n",
        "  For max-pooling backpropagation, the gradient of the pooling layer is only\n",
        "  backpropagated through the highest element of the input at the current window.\n",
        "  There is no gradient with respect to non-maximum values since they don't\n",
        "  affect the error. All other values get a gradient of zero. Based on:\n",
        "  https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers\n",
        "    \n",
        "  Args:\n",
        "    d_pool: Gradient respective to the pooling layer.\n",
        "    orig: The layer to which the max-pooling layer is connected.\n",
        "    f: Size of the pooling window.\n",
        "    s: Stride of the pooling layer\n",
        "  Returns:\n",
        "    The gradient of the pooling layer.\n",
        "  '''\n",
        "  (n_c, orig_dim, _) = orig.shape\n",
        "\n",
        "  dout = np.zeros(orig.shape)\n",
        "\n",
        "  # Iterate through channels\n",
        "  for curr_c in range(n_c):\n",
        "    # Track y index of current (maxpooling) and output (previous) layer.\n",
        "    curr_y = 0\n",
        "    out_y = 0\n",
        "\n",
        "    # Move window through rows of image\n",
        "    while curr_y + f <= orig_dim:\n",
        "      # Track x index of current (maxpooling) and output (previous) layer.\n",
        "      curr_x = 0\n",
        "      out_x = 0\n",
        "\n",
        "      # Move window through columns of the image\n",
        "      while curr_x + f <= orig_dim:\n",
        "        # Obtain index of the largest value (max pooling)\n",
        "        (a, b) = nanargmax(orig[curr_c, curr_y:curr_y+f, curr_x:curr_x+f])\n",
        "        dout[curr_c, curr_y+a, curr_x+b] = dpool[curr_c, out_y, out_x]\n",
        "\n",
        "        curr_x += s\n",
        "        out_x += 1\n",
        "      curr_y += s\n",
        "      out_y += 1\n",
        "\n",
        "  return dout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zolJUiIb3NTT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Network"
      ]
    },
    {
      "metadata": {
        "id": "OGxyugCZ3JPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def network(image, label, params, conv_s, pool_f, pool_s):\n",
        "  '''Runs an iteration of the full network\n",
        "    \n",
        "  Args:\n",
        "    image: An image from the training set\n",
        "    label: A one-hot encoded label.\n",
        "    params: A list containing the filters, weights and biases.\n",
        "    conv_s: The stride of the convolutional network.\n",
        "    pool_f: The size of the maxpooling layer.\n",
        "    pool_s: The stride of the maxpooling layer.\n",
        "  Returns:\n",
        "    The cost after the training is complete\n",
        "  '''\n",
        "  [f1, f2, w1, w2, b1, b2, b3, b4] = params \n",
        "\n",
        "  # Feed forward\n",
        "  \n",
        "  # First convolutional layer + ReLU activation\n",
        "  conv1 = convolution(image, f1, b1)\n",
        "  conv1[conv1<=0] = 0\n",
        "\n",
        "  # Second convolutional layer + ReLU activation\n",
        "  conv2 = convolution(conv1, f2, b2)\n",
        "  conv2[conv2<=0] = 0\n",
        "\n",
        "  # Maxpooling\n",
        "  pooled = maxpool(conv2, pool_f, pool_s)\n",
        "\n",
        "  # Flattening\n",
        "  (nf2, dim2, _) = pooled.shape\n",
        "  fc = pooled.reshape((nf2 * dim2 * dim2, 1))\n",
        "  \n",
        "  # First dense layer + ReLU activation\n",
        "  w1 = gpuarray.to_gpu(w1)\n",
        "  fc = gpuarray.to_gpu(fc)\n",
        "  z = gpuarray.dot(w1, fc)\n",
        "  z = z.get() + b3\n",
        "  z[z<=0] = 0\n",
        "\n",
        "  # Output layer (dense) + softmax activation to predict probabilities\n",
        "  w2 = gpuarray.to_gpu(w2)\n",
        "  z = gpuarray.to_gpu(z)\n",
        "  out = gpuarray.dot(w2, z)\n",
        "  out = out.get() + b4\n",
        "  probs = softmax(out)\n",
        "\n",
        "  # Calculate loss with categorical crossentropy\n",
        "  loss = categoricalCrossEntropy(probs, label)\n",
        "\n",
        "  # Calculate error\n",
        "  dout = probs - label \n",
        "  \n",
        "  # Last layer -> first network layer\n",
        "  # Loss gradient respective to the final dense layer weights and biases\n",
        "  z = z.get()\n",
        "  dw2 = dout.dot(z.T)\n",
        "  db4 = np.sum(dout, axis = 1).reshape(b4.shape)\n",
        "\n",
        "  # First network layer -> flatten layer\n",
        "  # Loss gradient respective to the first dense layer weights and biases.\n",
        "  # Additionally backpropagate through ReLU.\n",
        "  w2 = w2.get()\n",
        "  fc = fc.get()\n",
        "  dz = w2.T.dot(dout)\n",
        "  dz[z<=0] = 0\n",
        "  dw1 = dz.dot(fc.T)\n",
        "  db3 = np.sum(dz, axis = 1).reshape(b3.shape)\n",
        "\n",
        "  # Flatten layer -> pooling layer\n",
        "  w1 = w1.get()\n",
        "  dfc = w1.T.dot(dz)\n",
        "  # Reshape for correct dimensions of pooling layer\n",
        "  dpool = dfc.reshape(pooled.shape)\n",
        "\n",
        "  # Pooling layer -> second convolutional\n",
        "  # Backpropagate through max-pooling layer and ReLU\n",
        "  dconv2 = maxpoolBackward(dpool, conv2, pool_f, pool_s)\n",
        "  dconv2[conv2<=0] = 0\n",
        "\n",
        "  # Second convolutional -> first convolutional\n",
        "  # Backpropagate through convolution and ReLU\n",
        "  dconv1, df2, db2 = convolutionBackward(dconv2, conv1, f2, conv_s)\n",
        "  dconv1[conv1<=0] = 0\n",
        "\n",
        "  # First convolutional -> input image\n",
        "  dimage, df1, db1 = convolutionBackward(dconv1, image, f1, conv_s)\n",
        "  \n",
        "  # Get gradients\n",
        "  grads = [df1, df2, dw1, dw2, db1, db2, db3, db4] \n",
        "\n",
        "  # Return gradients and loss\n",
        "  return grads, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeZyU8Guvj-O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Optimization"
      ]
    },
    {
      "metadata": {
        "id": "RojagBE3vlvp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def adam(batch, num_classes, lr, dim, n_c, beta1, beta2, params, cost):\n",
        "  '''Runs the Adam optimizer of the convolutional neural network\n",
        "    \n",
        "  Args:\n",
        "    num_classes: Number of classes for the problem. For this problem, 10 digits.\n",
        "    lr: Learning rate, also knowns as alpha for Adam.\n",
        "    n_c: Number of channels of the input images.\n",
        "    beta1: Exponential decay rate of first momentum.\n",
        "    beta2: Exponential decay rate of second momentum.\n",
        "    params: A list containing the filters, weights and biases.\n",
        "    cost: A list of costs.\n",
        "\n",
        "  Returns:\n",
        "    The updated parameters and the updated list of costs.\n",
        "  '''\n",
        "  [f1, f2, w1, w2, b1, b2, b3, b4] = params\n",
        "\n",
        "  # Get input and reshape \n",
        "  X = batch[:,0:-1]\n",
        "  X = X.reshape(len(batch), n_c, dim, dim)\n",
        "  Y = batch[:,-1]\n",
        "\n",
        "  # Cost of this iteration\n",
        "  cost_ = 0\n",
        "\n",
        "  batch_size = len(batch)\n",
        "\n",
        "  # Initialize gradients \n",
        "  df1 = np.zeros(f1.shape)\n",
        "  df2 = np.zeros(f2.shape)\n",
        "  dw1 = np.zeros(w1.shape)\n",
        "  dw2 = np.zeros(w2.shape)\n",
        "  db1 = np.zeros(b1.shape)\n",
        "  db2 = np.zeros(b2.shape)\n",
        "  db3 = np.zeros(b3.shape)\n",
        "  db4 = np.zeros(b4.shape)\n",
        "\n",
        "  # Initialize momentum\n",
        "  v1 = np.zeros(f1.shape)\n",
        "  v2 = np.zeros(f2.shape)\n",
        "  v3 = np.zeros(w1.shape)\n",
        "  v4 = np.zeros(w2.shape)\n",
        "  bv1 = np.zeros(b1.shape)\n",
        "  bv2 = np.zeros(b2.shape)\n",
        "  bv3 = np.zeros(b3.shape)\n",
        "  bv4 = np.zeros(b4.shape)\n",
        "\n",
        "  # Initialize RMS param\n",
        "  s1 = np.zeros(f1.shape)\n",
        "  s2 = np.zeros(f2.shape)\n",
        "  s3 = np.zeros(w1.shape)\n",
        "  s4 = np.zeros(w2.shape)\n",
        "  bs1 = np.zeros(b1.shape)\n",
        "  bs2 = np.zeros(b2.shape)\n",
        "  bs3 = np.zeros(b3.shape)\n",
        "  bs4 = np.zeros(b4.shape)\n",
        "\n",
        "  # Iterate through elements\n",
        "  for i in range(batch_size):\n",
        "    # Extract the element\n",
        "    x = X[i]\n",
        "    \n",
        "    # One-hot encoding\n",
        "    y = np.eye(num_classes)[int(Y[i])].reshape(num_classes, 1)\n",
        "\n",
        "    # Get gradient and loss\n",
        "    grads, loss = network(x, y, params, 1, 2, 4)\n",
        "    [df1_, df2_, dw1_, dw2_, db1_, db2_, db3_, db4_] = grads\n",
        "\n",
        "    # Update gradients (get cumulative)\n",
        "    df1 += df1_\n",
        "    db1 += db1_\n",
        "    \n",
        "    df2 += df2_\n",
        "    db2 += db2_\n",
        "    \n",
        "    dw1 += dw1_\n",
        "    db3 += db3_\n",
        "    \n",
        "    dw2 += dw2_\n",
        "    db4 += db4_\n",
        "\n",
        "    cost_+= loss\n",
        "\n",
        "  # Update momentum, RMSProp and combine to perform Adam update\n",
        "  v1 = beta1*v1 + (1-beta1)*df1/batch_size \n",
        "  s1 = beta2*s1 + (1-beta2)*(df1/batch_size)**2 \n",
        "  f1 -= lr * v1/np.sqrt(s1+1e-7) \n",
        "\n",
        "  bv1 = beta1*bv1 + (1-beta1)*db1/batch_size\n",
        "  bs1 = beta2*bs1 + (1-beta2)*(db1/batch_size)**2\n",
        "  b1 -= lr * bv1/(np.sqrt(bs1)+1e-7)\n",
        "\n",
        "  v2 = beta1*v2 + (1-beta1)*df2/batch_size\n",
        "  s2 = beta2*s2 + (1-beta2)*(df2/batch_size)**2\n",
        "  f2 -= lr * v2/(np.sqrt(s2)+1e-7)\n",
        "\n",
        "  bv2 = beta1*bv2 + (1-beta1) * db2/batch_size\n",
        "  bs2 = beta2*bs2 + (1-beta2)*(db2/batch_size)**2\n",
        "  b2 -= lr * bv2/(np.sqrt(bs2)+1e-7)\n",
        "\n",
        "  v3 = beta1*v3 + (1-beta1) * dw1/batch_size\n",
        "  s3 = beta2*s3 + (1-beta2)*(dw1/batch_size)**2\n",
        "  w1 -= lr * v3/(np.sqrt(s3)+1e-7)\n",
        "\n",
        "  bv3 = beta1*bv3 + (1-beta1) * db3/batch_size\n",
        "  bs3 = beta2*bs3 + (1-beta2)*(db3/batch_size)**2\n",
        "  b3 -= lr * bv3/(np.sqrt(bs3)+1e-7)\n",
        "\n",
        "  v4 = beta1*v4 + (1-beta1) * dw2/batch_size\n",
        "  s4 = beta2*s4 + (1-beta2)*(dw2/batch_size)**2\n",
        "  w2 -= lr * v4/(np.sqrt(s4)+1e-7)\n",
        "\n",
        "  bv4 = beta1*bv4 + (1-beta1)*db4/batch_size\n",
        "  bs4 = beta2*bs4 + (1-beta2)*(db4/batch_size)**2\n",
        "  b4 -= lr * bv4/(np.sqrt(bs4)+1e-7)\n",
        "\n",
        "  # Update cost and store\n",
        "  cost_ = cost_/batch_size\n",
        "  cost.append(cost_)\n",
        "\n",
        "  # Parameters\n",
        "  params = [f1, f2, w1, w2, b1, b2, b3, b4]\n",
        "\n",
        "  return params, cost\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v7Nfsk0ivgOz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "kjyFn9e8vdX-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(num_classes = 10, lr = 0.01, beta1 = 0.95, beta2 = 0.99, img_dim = 28, img_depth = 1, f = 2, num_filt1 = 4, num_filt2 = 4, batch_size = 64, num_epochs = 1, save_path = 'params.pkl'):\n",
        "  '''Executes the training of the optimization.\n",
        "  \n",
        "  This function loads the features and labels, initializes the weights and\n",
        "  filters, and splits the data in batches to later be used in every iteration\n",
        "  that calls the Adam optimization. Reports the results in a dump file.\n",
        "    \n",
        "  Args:\n",
        "    num_classes: Number of classes for the problem. For this problem, 10 digits.\n",
        "    lr: Learning rate, also knowns as alpha for Adam.\n",
        "    beta1: Exponential decay rate of first momentum.\n",
        "    beta2: Exponential decay rate of second momentum.\n",
        "    img_dim: Dimensions of the image, assuming it is a square image.\n",
        "    img_depth: Number of channels of the image\n",
        "    f: Size of the filter.\n",
        "    num_filt1: Number of filters in the first convolutional layer.\n",
        "    num_filt2: Number of filters in the second convolutional layer.\n",
        "    batch_size: Number of training samples to be used in every iteration.\n",
        "    num_epochs: Times to iterate through the whole training set.\n",
        "    save_path: Path to save the parameters.\n",
        "\n",
        "  Returns:\n",
        "    A list of costs.\n",
        "  '''\n",
        "  \n",
        "  # Number of training samples\n",
        "  m = 50000\n",
        "\n",
        "  # Get training data\n",
        "  X = extract_data('train-images-idx3-ubyte.gz', m, img_dim)\n",
        "  y = extract_labels('train-labels-idx1-ubyte.gz', m).reshape(m, 1)\n",
        "  \n",
        "  # Normalize data\n",
        "  X -= int(np.mean(X))\n",
        "  X /= int(np.std(X))\n",
        "  train_data = np.hstack((X, y))\n",
        "\n",
        "  np.random.shuffle(train_data)\n",
        "\n",
        "  # Initialize filters. The are the number of dimensions x number of channels x the size of the filter\n",
        "  f1 = (num_filt1, img_depth, f, f)\n",
        "  f2 = (num_filt2, num_filt1, f, f)\n",
        "  \n",
        "  # Weights of the neural network. 2500 (flatten) -> 128 -> 10\n",
        "  w1 = (128, 196)\n",
        "  w2 = (10, 128)\n",
        "  \n",
        "  # Initialize filter and weight values\n",
        "  f1 = initializeFilter(f1)\n",
        "  f2 = initializeFilter(f2)\n",
        "  w1 = initializeWeight(w1)\n",
        "  w2 = initializeWeight(w2)\n",
        "\n",
        "  # Initialize the bias with a value of 0\n",
        "  b1 = np.zeros((f1.shape[0],1))\n",
        "  b2 = np.zeros((f2.shape[0],1))\n",
        "  b3 = np.zeros((w1.shape[0],1))\n",
        "  b4 = np.zeros((w2.shape[0],1))\n",
        "\n",
        "  # Combine params\n",
        "  params = [f1, f2, w1, w2, b1, b2, b3, b4]\n",
        "  cost = []\n",
        "\n",
        "  print(\"LR:\" +str(lr)+ \", Batch Size:\" + str(batch_size))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    # Shuffle data and divide in batches\n",
        "    np.random.shuffle(train_data)\n",
        "    batches = [train_data[k: k+batch_size] for k in range(0, train_data.shape[0], batch_size)]\n",
        "\n",
        "    # Feed every batch to the Adam Gradient Descent algorithm\n",
        "    t = tqdm(batches, position=0)\n",
        "    for x, batch in enumerate(t):\n",
        "        params, cost = adam(batch, num_classes, lr, img_dim, img_depth, beta1, beta2, params, cost)\n",
        "        t.set_description(\"Cost: %.2f\" % (cost[-1]))\n",
        "\n",
        "  # Save relevant data in the file\n",
        "  data = [params, cost]\n",
        "  with open(save_path, 'wb') as file:\n",
        "      pickle.dump(data, file)\n",
        "\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7zmhCegYLTRz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ]
    },
    {
      "metadata": {
        "id": "bsRV0dkjLVDv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(image, f1, f2, w1, w2, b1, b2, b3, b4):\n",
        "  '''Makes predictions from given parameters.\n",
        "\n",
        "  Args:\n",
        "    image: An image of 1x28x28 of a digit.\n",
        "    f1: A list of filters of the first convolutional layer.\n",
        "    f2: A list of filters of the second convolutional layer.\n",
        "    w1: The weights of the first neural layer.\n",
        "    w2: The weights of the second neural layer.\n",
        "    b1-b4: The biases of the 4 layers.\n",
        "\n",
        "  Returns:\n",
        "    A tuple with the digit predicted and the probability of the image being that digit.\n",
        "  '''\n",
        "\n",
        "  # First convolution\n",
        "  conv1 = convolution(image, f1, b1)\n",
        "  conv1[conv1<=0] = 0\n",
        "\n",
        "  # Second convolution\n",
        "  conv2 = convolution(conv1, f2, b2) \n",
        "  conv2[conv2<=0] = 0 \n",
        "\n",
        "  # Max poooling\n",
        "  pooled = maxpool(conv2, 2, 4) \n",
        "\n",
        "  # Flattening\n",
        "  (nf2, dim2, _) = pooled.shape\n",
        "  fc = pooled.reshape((nf2 * dim2 * dim2, 1))\n",
        "\n",
        "  # First dense layer\n",
        "  w1 = gpuarray.to_gpu(w1)\n",
        "  fc = gpuarray.to_gpu(fc)\n",
        "  z = gpuarray.dot(w1, fc)\n",
        "  z = z.get() + b3\n",
        "  z[z<=0] = 0\n",
        "\n",
        "  # Output layer\n",
        "  w2 = gpuarray.to_gpu(w2)\n",
        "  z = gpuarray.to_gpu(z)\n",
        "  out = gpuarray.dot(w2, z)\n",
        "  out = out.get() + b4\n",
        "  probs = softmax(out)\n",
        "\n",
        "  # Return the maximum value given the probabilities\n",
        "  return np.argmax(probs), np.max(probs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_jDLuel6J_5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Main"
      ]
    },
    {
      "metadata": {
        "id": "FUOw8xuxZSdD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "cost = train()\n",
        "\n",
        "# Load parameters from file\n",
        "params, cost = pickle.load(open('params.pkl', 'rb'))\n",
        "[f1, f2, w1, w2, b1, b2, b3, b4] = params\n",
        "\n",
        "# Draw plot of loss\n",
        "plt.plot(cost, 'r')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost')\n",
        "plt.legend('Loss', loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Get test data\n",
        "m = 10000\n",
        "X = extract_data('t10k-images-idx3-ubyte.gz', m, 28)\n",
        "y = extract_labels('t10k-labels-idx1-ubyte.gz', m).reshape(m,1)\n",
        "\n",
        "# Normalize data\n",
        "X -= int(np.mean(X))\n",
        "X /= int(np.std(X))\n",
        "\n",
        "test_data = np.hstack((X, y))\n",
        "\n",
        "# Extract features and be sure that the structure is 1x28x28\n",
        "X = test_data[:,0:-1]\n",
        "X = X.reshape(len(test_data), 1, 28, 28)\n",
        "\n",
        "# Extract label\n",
        "y = test_data[:,-1]\n",
        "\n",
        "# Counter of correct predictions\n",
        "correct = 0\n",
        "print(\"Computing accuracy over test set:\")\n",
        "\n",
        "t2 = tqdm(range(len(X)), position=0)\n",
        "for i in t2:\n",
        "    x = X[i]\n",
        "    \n",
        "    # Get a prediction. Here prob would be useful for other metrics.\n",
        "    pred, prob = predict(x, f1, f2, w1, w2, b1, b2, b3, b4)\n",
        "\n",
        "    if pred == y[i]:\n",
        "        correct+=1\n",
        "\n",
        "    t2.set_description(\"Acc:%0.2f%%\" % (float(correct/(i+1))*100))\n",
        "\n",
        "print(\"Overall Accuracy: %.2f\" % (float(correct/len(test_data)*100)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpFHfdSyr0sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}